# EchoMind Ingestor Service Configuration
# Used by docker-compose cluster deployment
#
# This service processes documents uploaded via connectors:
# - Downloads files from MinIO
# - Extracts content using nv-ingest (PDF, DOCX, images, audio, etc.)
# - Chunks text using tokenizer-based splitter
# - Generates embeddings via Embedder gRPC service
# - Stores vectors in Qdrant

# Service Settings
INGESTOR_ENABLED=true
INGESTOR_HEALTH_PORT=8080

# Database
INGESTOR_DATABASE_URL=postgresql+asyncpg://postgres:postgres@echomind-postgres:5432/echomind
INGESTOR_DATABASE_ECHO=false

# NATS
INGESTOR_NATS_URL=nats://echomind-nats:4222
INGESTOR_NATS_STREAM_NAME=ECHOMIND
INGESTOR_NATS_CONSUMER_NAME=ingestor-consumer

# MinIO
INGESTOR_MINIO_ENDPOINT=echomind-minio:9000
INGESTOR_MINIO_ACCESS_KEY=minioadmin
INGESTOR_MINIO_SECRET_KEY=minioadmin
INGESTOR_MINIO_SECURE=false
INGESTOR_MINIO_BUCKET=echomind-documents

# Qdrant
INGESTOR_QDRANT_HOST=echomind-qdrant
INGESTOR_QDRANT_PORT=6333
# INGESTOR_QDRANT_API_KEY=  # Uncomment if Qdrant requires authentication

# Embedder gRPC
INGESTOR_EMBEDDER_HOST=echomind-embedder
INGESTOR_EMBEDDER_PORT=50051
INGESTOR_EMBEDDER_TIMEOUT=30.0

# nv-ingest Extraction Settings
# Extraction method: pdfium (fast), pdfium_hybrid, nemotron_parse (requires NIM)
INGESTOR_EXTRACT_METHOD=pdfium

# Chunking Settings (in TOKENS, not characters)
INGESTOR_CHUNK_SIZE=512
INGESTOR_CHUNK_OVERLAP=50

# Tokenizer for chunking (HuggingFace model)
# Use gpt2 for local dev, meta-llama/Llama-3.2-1B for production
INGESTOR_TOKENIZER=gpt2

# Optional NIMs (set to true if NIMs are deployed)
# YOLOX: Table and chart detection
INGESTOR_YOLOX_ENABLED=false
INGESTOR_YOLOX_ENDPOINT=http://yolox-nim:8000

# Riva: Audio transcription
INGESTOR_RIVA_ENABLED=false
INGESTOR_RIVA_ENDPOINT=http://riva:50051

# Retry Settings
INGESTOR_MAX_RETRIES=3
INGESTOR_RETRY_BASE_DELAY=1.0

# Logging
INGESTOR_LOG_LEVEL=INFO
