// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.11.0
//   protoc               v5.29.3
// source: public/llm.proto

/* eslint-disable */
import type { PaginationRequest, PaginationResponse } from "../common";

/** LLM provider types */
export enum LLMProvider {
  LLM_PROVIDER_UNSPECIFIED = "LLM_PROVIDER_UNSPECIFIED",
  /** LLM_PROVIDER_TGI - Text Generation Inference */
  LLM_PROVIDER_TGI = "LLM_PROVIDER_TGI",
  /** LLM_PROVIDER_VLLM - vLLM */
  LLM_PROVIDER_VLLM = "LLM_PROVIDER_VLLM",
  /** LLM_PROVIDER_OPENAI - OpenAI API */
  LLM_PROVIDER_OPENAI = "LLM_PROVIDER_OPENAI",
  /** LLM_PROVIDER_ANTHROPIC - Anthropic API */
  LLM_PROVIDER_ANTHROPIC = "LLM_PROVIDER_ANTHROPIC",
  /** LLM_PROVIDER_OLLAMA - Ollama (local dev) */
  LLM_PROVIDER_OLLAMA = "LLM_PROVIDER_OLLAMA",
  UNRECOGNIZED = "UNRECOGNIZED",
}

/** LLM configuration */
export interface LLM {
  id: number;
  name: string;
  provider: LLMProvider;
  modelId: string;
  endpoint: string;
  /** Don't expose actual key, just whether it's set */
  hasApiKey: boolean;
  maxTokens: number;
  temperature: number;
  isDefault: boolean;
  isActive: boolean;
  creationDate?: Date | undefined;
  lastUpdate?: Date | undefined;
}

/** Request to create a new LLM config */
export interface CreateLLMRequest {
  name: string;
  provider: LLMProvider;
  modelId: string;
  endpoint: string;
  apiKey?: string | undefined;
  maxTokens: number;
  temperature: number;
  isDefault: boolean;
  isActive: boolean;
}

/** Request to update an LLM config */
export interface UpdateLLMRequest {
  id: number;
  name?: string | undefined;
  provider?: LLMProvider | undefined;
  modelId?: string | undefined;
  endpoint?: string | undefined;
  apiKey?: string | undefined;
  maxTokens?: number | undefined;
  temperature?: number | undefined;
  isDefault?: boolean | undefined;
  isActive?: boolean | undefined;
}

/** Response for GET /llms */
export interface ListLLMsRequest {
  pagination?: PaginationRequest | undefined;
  isActive?: boolean | undefined;
}

export interface ListLLMsResponse {
  llms: LLM[];
  pagination?: PaginationResponse | undefined;
}

/** Response for GET /llms/{id} */
export interface GetLLMResponse {
  llm?: LLM | undefined;
}

/** Response for POST /llms/{id}/test */
export interface TestLLMResponse {
  success: boolean;
  message: string;
  latencyMs?: number | undefined;
}
