// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.11.0
//   protoc               v5.29.3
// source: public/llm.proto

/* eslint-disable */
import type { PaginationRequest, PaginationResponse } from "../common";

/**
 * LLM provider types
 * Simplified: TGI/vLLM/OpenAI/Ollama all use OpenAI-compatible API format
 */
export enum LLMProvider {
  LLM_PROVIDER_UNSPECIFIED = "LLM_PROVIDER_UNSPECIFIED",
  /** LLM_PROVIDER_OPENAI_COMPATIBLE - Any OpenAI-compatible API (TGI, vLLM, OpenAI, Ollama, etc.) */
  LLM_PROVIDER_OPENAI_COMPATIBLE = "LLM_PROVIDER_OPENAI_COMPATIBLE",
  /** LLM_PROVIDER_ANTHROPIC - Anthropic Messages API (streaming, pay-per-token) */
  LLM_PROVIDER_ANTHROPIC = "LLM_PROVIDER_ANTHROPIC",
  /** LLM_PROVIDER_ANTHROPIC_TOKEN - Claude CLI with Max subscription OAuth token */
  LLM_PROVIDER_ANTHROPIC_TOKEN = "LLM_PROVIDER_ANTHROPIC_TOKEN",
  UNRECOGNIZED = "UNRECOGNIZED",
}

/** LLM configuration */
export interface LLM {
  id: number;
  name: string;
  provider: LLMProvider;
  modelId: string;
  endpoint: string;
  /** Don't expose actual key, just whether it's set */
  hasApiKey: boolean;
  maxTokens: number;
  temperature: number;
  isDefault: boolean;
  isActive: boolean;
  creationDate?: Date | undefined;
  lastUpdate?: Date | undefined;
}

/** Request to create a new LLM config */
export interface CreateLLMRequest {
  name: string;
  provider: LLMProvider;
  modelId: string;
  endpoint: string;
  apiKey?: string | undefined;
  maxTokens: number;
  temperature: number;
  isDefault: boolean;
  isActive: boolean;
}

/** Request to update an LLM config */
export interface UpdateLLMRequest {
  id: number;
  name?: string | undefined;
  provider?: LLMProvider | undefined;
  modelId?: string | undefined;
  endpoint?: string | undefined;
  apiKey?: string | undefined;
  maxTokens?: number | undefined;
  temperature?: number | undefined;
  isDefault?: boolean | undefined;
  isActive?: boolean | undefined;
}

/** Response for GET /llms */
export interface ListLLMsRequest {
  pagination?: PaginationRequest | undefined;
  isActive?: boolean | undefined;
}

export interface ListLLMsResponse {
  llms: LLM[];
  pagination?: PaginationResponse | undefined;
}

/** Response for GET /llms/{id} */
export interface GetLLMResponse {
  llm?: LLM | undefined;
}

/** Response for POST /llms/{id}/test */
export interface TestLLMResponse {
  success: boolean;
  message: string;
  latencyMs?: number | undefined;
}
