syntax = "proto3";

package echomind.public;

option go_package = "echomind/proto/public";

import "google/protobuf/timestamp.proto";
import "common.proto";

// LLM provider types
enum LLMProvider {
  LLM_PROVIDER_UNSPECIFIED = 0;
  LLM_PROVIDER_TGI = 1;        // Text Generation Inference
  LLM_PROVIDER_VLLM = 2;       // vLLM
  LLM_PROVIDER_OPENAI = 3;     // OpenAI API
  LLM_PROVIDER_ANTHROPIC = 4;  // Anthropic API
  LLM_PROVIDER_OLLAMA = 5;     // Ollama (local dev)
}

// LLM configuration
message LLM {
  int32 id = 1;
  string name = 2;
  LLMProvider provider = 3;
  string model_id = 4;
  string endpoint = 5;
  bool has_api_key = 6;  // Don't expose actual key, just whether it's set
  int32 max_tokens = 7;
  double temperature = 8;
  bool is_default = 9;
  bool is_active = 10;
  google.protobuf.Timestamp creation_date = 11;
  google.protobuf.Timestamp last_update = 12;
}

// Request to create a new LLM config
message CreateLLMRequest {
  string name = 1;
  LLMProvider provider = 2;
  string model_id = 3;
  string endpoint = 4;
  optional string api_key = 5;
  int32 max_tokens = 6;
  double temperature = 7;
  bool is_default = 8;
  bool is_active = 9;
}

// Request to update an LLM config
message UpdateLLMRequest {
  int32 id = 1;
  optional string name = 2;
  optional LLMProvider provider = 3;
  optional string model_id = 4;
  optional string endpoint = 5;
  optional string api_key = 6;
  optional int32 max_tokens = 7;
  optional double temperature = 8;
  optional bool is_default = 9;
  optional bool is_active = 10;
}

// Response for GET /llms
message ListLLMsRequest {
  echomind.common.PaginationRequest pagination = 1;
  optional bool is_active = 2;
}

message ListLLMsResponse {
  repeated LLM llms = 1;
  echomind.common.PaginationResponse pagination = 2;
}

// Response for GET /llms/{id}
message GetLLMResponse {
  LLM llm = 1;
}

// Response for POST /llms/{id}/test
message TestLLMResponse {
  bool success = 1;
  string message = 2;
  optional int32 latency_ms = 3;
}
