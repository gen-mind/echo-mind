# docker-compose-observability.yml
# Observability stack: Grafana + Prometheus + Loki + Alloy + exporters
# Gated by ENABLE_OBSERVABILITY=true in .env (via --profile observability)
# Zero changes to existing EchoMind services.

services:
  # ============================================
  # METRICS BACKEND
  # ============================================
  prometheus:
    image: prom/prometheus:v3.5.1
    container_name: echomind-prometheus
    profiles: ["observability"]
    volumes:
      - ${CONFIG_PATH}/observability/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=30d"
      - "--web.enable-lifecycle"
      - "--web.enable-otlp-receiver"   # Future-proof: accept OTLP metrics for Phase 2+
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 15s
      timeout: 5s
      start_period: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - backend
    labels:
      - "traefik.enable=true"
      # Local mode
      - "traefik.http.routers.prometheus.rule=Host(`prometheus.${DOMAIN}`)"
      - "traefik.http.routers.prometheus.entrypoints=web"
      - "traefik.http.services.prometheus.loadbalancer.server.port=9090"

  # ============================================
  # LOGS BACKEND
  # ============================================
  loki:
    image: grafana/loki:3.6.0
    container_name: echomind-loki
    profiles: ["observability"]
    volumes:
      - ${CONFIG_PATH}/observability/loki/loki-config.yml:/etc/loki/loki-config.yml:ro
      - loki_data:/loki
    command: -config.file=/etc/loki/loki-config.yml
    restart: unless-stopped
    networks:
      - backend

  # ============================================
  # LOG COLLECTOR (replaces deprecated Promtail)
  # ============================================
  alloy:
    image: grafana/alloy:v1.11.3
    container_name: echomind-alloy
    profiles: ["observability"]
    volumes:
      - ${CONFIG_PATH}/observability/alloy/config.alloy:/etc/alloy/config.alloy:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command:
      - run
      - /etc/alloy/config.alloy
      - --storage.path=/var/lib/alloy/data
      - --server.http.listen-addr=0.0.0.0:12345
    depends_on:
      loki:
        condition: service_started
    restart: unless-stopped
    networks:
      - backend

  # ============================================
  # NATS METRICS EXPORTER
  # (NATS exposes JSON at :8222, not Prometheus format)
  # ============================================
  nats-exporter:
    image: natsio/prometheus-nats-exporter:latest
    container_name: echomind-nats-exporter
    profiles: ["observability"]
    command: -varz -jsz all http://nats:8222
    depends_on:
      - nats
    restart: unless-stopped
    networks:
      - backend

  # ============================================
  # POSTGRESQL METRICS EXPORTER
  # (PostgreSQL has no native /metrics endpoint)
  # ============================================
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: echomind-postgres-exporter
    profiles: ["observability"]
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/postgres?sslmode=disable"
    depends_on:
      - postgres
    restart: unless-stopped
    networks:
      - backend

  # ============================================
  # CONTAINER METRICS
  # ============================================
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.51.0
    container_name: echomind-cadvisor
    profiles: ["observability"]
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    privileged: true
    restart: unless-stopped
    networks:
      - backend

  # ============================================
  # HOST METRICS
  # ============================================
  node-exporter:
    image: prom/node-exporter:v1.9.0
    container_name: echomind-node-exporter
    profiles: ["observability"]
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--path.rootfs=/rootfs"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
    restart: unless-stopped
    networks:
      - backend

  # ============================================
  # VISUALIZATION + ALERTING
  # ============================================
  grafana:
    image: grafana/grafana:12.3.2
    container_name: echomind-grafana
    profiles: ["observability"]
    volumes:
      - ${CONFIG_PATH}/observability/grafana/provisioning:/etc/grafana/provisioning:ro
      - ${CONFIG_PATH}/observability/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana_data:/var/lib/grafana
    environment:
      # --- Server ---
      - GF_SERVER_ROOT_URL=${PROTOCOL}://grafana.${DOMAIN}
      - GF_SERVER_DOMAIN=grafana.${DOMAIN}

      # --- Admin ---
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}

      # --- Authentik OIDC (disabled until GRAFANA_OAUTH_ENABLED=true in .env) ---
      - GF_AUTH_GENERIC_OAUTH_ENABLED=${GRAFANA_OAUTH_ENABLED:-false}
      - GF_AUTH_GENERIC_OAUTH_NAME=Authentik
      - GF_AUTH_GENERIC_OAUTH_CLIENT_ID=${GRAFANA_OAUTH_CLIENT_ID:-}
      - GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=${GRAFANA_OAUTH_CLIENT_SECRET:-}
      - GF_AUTH_GENERIC_OAUTH_AUTH_URL=${AUTHENTIK_URL}/application/o/authorize/
      - GF_AUTH_GENERIC_OAUTH_TOKEN_URL=${AUTHENTIK_URL}/application/o/token/
      - GF_AUTH_GENERIC_OAUTH_API_URL=${AUTHENTIK_URL}/application/o/userinfo/
      - GF_AUTH_GENERIC_OAUTH_SCOPES=openid profile email grafana
      - GF_AUTH_GENERIC_OAUTH_USE_PKCE=true

      # --- Role Mapping ---
      - GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH=contains(info.groups[*], 'echomind-admins') && 'GrafanaAdmin' || 'Viewer'
      - GF_AUTH_GENERIC_OAUTH_ALLOW_ASSIGN_GRAFANA_ADMIN=true
      - GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_STRICT=true

      # --- Logout ---
      - GF_AUTH_SIGNOUT_REDIRECT_URL=${AUTHENTIK_URL}/application/o/grafana/end-session/

      # --- Login behavior ---
      - GF_AUTH_OAUTH_AUTO_LOGIN=false
      - GF_AUTH_DISABLE_LOGIN_FORM=false
      - GF_AUTH_BASIC_ENABLED=true
      - GF_AUTH_ANONYMOUS_ENABLED=false

      # --- Organization ---
      - GF_USERS_AUTO_ASSIGN_ORG=true
      - GF_USERS_AUTO_ASSIGN_ORG_ID=1
      - GF_USERS_AUTO_ASSIGN_ORG_ROLE=Viewer
      - GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP=true

      # --- Security ---
      - GF_SECURITY_COOKIE_SECURE=true
      - GF_SECURITY_COOKIE_SAMESITE=lax

      # --- Analytics ---
      - GF_ANALYTICS_REPORTING_ENABLED=false
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:3000/api/health || exit 1"]
      interval: 15s
      timeout: 5s
      start_period: 15s
      retries: 3
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_started
    restart: unless-stopped
    networks:
      - frontend
      - backend
    labels:
      - "traefik.enable=true"
      # Local mode
      - "traefik.http.routers.grafana.rule=Host(`grafana.${DOMAIN}`)"
      - "traefik.http.routers.grafana.entrypoints=web"
      - "traefik.http.services.grafana.loadbalancer.server.port=3000"

volumes:
  prometheus_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/prometheus
  loki_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/loki
  grafana_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/grafana

